{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b55a54-c1b4-4c86-b560-5ad8c713b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchmetrics import Accuracy\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b8275e-7ed8-48ac-b2e0-e9b0ae2d6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy(task=\"multiclass\", num_classes=5)\n",
    "\n",
    "# Load and modify ResNet34\n",
    "model = models.resnet34()\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(in_features=512, out_features=5, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd57c14-9b22-40d1-9a56-61e59930e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device(num):\n",
    "    \"\"\"Pick GPU if available else cpu\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{num}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66c9fd-67a7-478f-8bd3-b684d2e8f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "# Define data augmentation transformations\n",
    "data_augmentation_transforms = T.Compose([\n",
    "    T.TimeMasking(time_mask_param=30),\n",
    "    T.FrequencyMasking(freq_mask_param=15),\n",
    "    T.TimeStretch()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817dbed4-2ea0-43cf-a73c-a206106d2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipsEarDataset(Dataset):\n",
    "    def __init__(self, annotation_file, audio_dir, transformation, target_sample_rate, num_samples, augmentations=None):\n",
    "        self.annotations = pd.read_csv(annotation_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augmentations:\n",
    "            signal = self.augmentations(signal)\n",
    "\n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"{self.annotations.iloc[index, 1]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f008c-9874-47d8-be99-d352e0f928bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = torchaudio.transforms.Spectrogram(n_fft=1024)\n",
    "\n",
    "ANNOTATIONS_FILE = \"Acoustic signal classification/AutoEncoder/AnnotationsAutoEncoder.csv\"\n",
    "AUDIO_DIR = \"Acoustic signal classification/AutoEncoder/Denoised_Audio50\"\n",
    "SAMPLE_RATE = 44100\n",
    "NUM_SAMPLES = 5 * SAMPLE_RATE\n",
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9c1e4-71d5-4d2b-a2c5-a7619db85e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the data augmentation only to the training dataset\n",
    "dataset = ShipsEarDataset(ANNOTATIONS_FILE, AUDIO_DIR, spectrogram, SAMPLE_RATE, NUM_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee71e2-8a50-47fb-af72-74363854866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(0.1 * len(sed))\n",
    "test_size = int(0.1 * len(sed))\n",
    "train_size = len(sed) - val_size - test_size\n",
    "train_ds, val_ds, test_ds = random_split(sed, [train_size, val_size, test_size])\n",
    "\n",
    "# Create augmented and non-augmented datasets\n",
    "train_ds.dataset.augmentations = data_augmentation_transforms\n",
    "val_ds.dataset.augmentations = None\n",
    "test_ds.dataset.augmentations = None\n",
    "\n",
    "train_dl = DataLoader(train_ds, \n",
    "                      batch_size=BATCH_SIZE, \n",
    "                      shuffle=True, \n",
    "                      num_workers=32, \n",
    "                      pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    num_workers=32, \n",
    "                    pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     num_workers=32, \n",
    "                     pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceef6a3-72ef-49c0-a51a-4a14607dd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device(0)\n",
    "\n",
    "train_loader = DeviceDataLoader(train_dl, device)\n",
    "val_loader = DeviceDataLoader(val_dl, device)\n",
    "test_loader = DeviceDataLoader(test_dl, device)\n",
    "accuracy = accuracy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64487f99-5c26-403d-be59-e2239d5f087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, valid_loader, num_epochs, device, clip_value=1.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    train_errors = []\n",
    "    valid_errors = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, targets in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_value)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_train += targets.size(0)\n",
    "            correct_train += predicted.eq(targets).sum().item()\n",
    "\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_errors.append(1 - train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        total_valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_valid_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_valid += targets.size(0)\n",
    "                correct_valid += predicted.eq(targets).sum().item()\n",
    "\n",
    "        valid_loss = total_valid_loss / len(valid_loader)\n",
    "        valid_accuracy = correct_valid / total_valid\n",
    "        valid_errors.append(1 - valid_accuracy)\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        torch.save(model.state_dict(), f'model_weights_{epoch+1}.pth')\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {total_train_loss:.4f} Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f} Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "    return train_errors, valid_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52f1c7-0a9e-4ce1-bb9c-e722b66b5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_errors, valid_errors = train_and_validate(model, train_loader, val_loader, num_epochs=100, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
